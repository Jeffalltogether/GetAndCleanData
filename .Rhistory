library(XML)
install.packages("XML")
library(XML)
fileUrl <- "http://www.w3schools.com/xml/simple.xml"
doc <- xmlTreeParse(fileUrl, useInternal=TRUE)
rootNode <- xmlRoot(doc)
xmlName(rootNode)
names(rootNode)
doc
head(doc)
doc
rootNode
xmlSApply(rootNode, xmlValue)
xpathSApply(rootNode, "//name", xmlValue)
xpathSApply(rootNode, "//price", xmlValue)
fileUrl <- "http://espn.go.com/nfl/team/_/name/bal/baltimore-ravens"
doc <- htmlTreeParse(fileUrl, useInternal=TRUE)
scores <- xpathSApply(doc, "//li[@class='score']", xmlValue)
teams <- xpathSApply(doc, "//li[@class='team-name']", xmlValue)
scores
teams
doc
scores <- xpathSApply(doc, "//li[@class='score']", xmlValue)
fileUrl <- "http://espn.go.com/nfl/team/schedule/_/name/bal/year/2014"
doc <- htmlTreeParse(fileUrl, useInternal=TRUE)  # need to use html rather than xml for website source-code
scores <- xpathSApply(doc, "//li[@class='score']", xmlValue)
teams <- xpathSApply(doc, "//li[@class='team-name']", xmlValue)
scores
teams
install.packages("jsonlite")
library(jsonlite)
jsonData <- fromJSON("https://api.github.com/users/jtleek/repos")
names(jsonData)
names(jsonData$owner)
names(jsonData$owner$login)
names(jsonData$owner$login)
jsonData <- fromJSON("https://api.github.com/users/jtleek/repos") # returns a structured data.frame
names(jsonData)
names(jsonData$owner)
jsonData$owner$login
head(jsonData)
myjson <- toJSON(iris, pretty=TRUE)
cat(myjson)
library(data.table)
install.packages("data.table")
library(data.table)
DF = data.frame(x=rnorm(9), y=rep(c("a","b","c"), each=3), z=rnorm(9))
head(DF,3)
DT = data.table(x=rnorm(9), y=rep(c("a","b","c"), each=3), z=rnorm(9))
head(DT,3)
tables() # see all the data tables in memory
DT[2,]
DT[DT$y="a",]
DT[DT$y=="a",]
DT[c(2,3)]
DT[,c(2,3)]
DT[,list(mean(x), sum(z))]
DT[,table(y)]
DT[,w:=z^2]
DT
DT2 <- DT
DT[,y:=2]
DT
DT2
DT[,m:={tmp <- (x+z); log2(tmp+5)}]
DT
DT[,a:x>0]
DT[,a:=x>0]
DT
DT[,b:=mean(x+w),by=a]
DT[,b:=mean(x+w),by=a] #takes the mean of x+w grouped by col `a`
DT
set seed(123)
set.seed(123)
DT <- data.table(x=sample(LETTERS[1:3], 1E5, TRUE))
DT[, .N, by=x]
DT = data.table(x=rep(c("a","b","c"), each=100, y=rnorm(300))
DT = data.table(x=rep(c("a","b","c"), each=100), y=rnorm(300))
setkey(DT,x)
DT['a']
DT1 <- data.table(x=c('a', 'a', 'b', 'dt1'), y=1:4)
DT2 <- data.table(x+c('a' 'b', 'dt2'),z=5:7)
DT2 <- data.table(x+c('a', 'b', 'dt2'),z=5:7)
DT2 <- data.table(x=c('a', 'b', 'dt2'),z=5:7)
DT1 <- data.table(x=c('a', 'a', 'b', 'dt1'), y=1:4)
DT2 <- data.table(x=c('a', 'b', 'dt2'),z=5:7)
setkey(DT1,x); setkey(DT2, x)
merge(DT1, DT2)
big_df <- data.frame(x=rnorm(1E6), y=rnorm(1E6))
file <- tempfile()
write.table(big_df, file=file, row.names=FALSE, col.names=TRUE, sep="\t", quote=FALSE)
system.time(fread(file))
system.time(read.table(file, header=TRUE, sep="\t"))
setwd("C:/Users/jeffthatcher/Cloud Drive/RRepos/CleaningData")
setwd("C:/Users/jeffthatcher/Cloud Drive/RRepos/GetCleanData")
if(!file.exists("quiz1")) {dir.create("quiz1")}
fileUrl <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv"
download.file(fileUrl, destfile = "./data/IdahoHousing.csv") #curl is necessary for MAC users getting data from https
list.files("./quiz1") # sort of like the ls() command, shows the files in the directory "./data"
dateDownloaded <- date()
dateDownloaded
fileUrl <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv"
download.file(fileUrl, destfile = "./quiz1/IdahoHousing.csv") #curl is necessary for MAC users getting data from https
list.files("./quiz1") # sort of like the ls() command, shows the files in the directory "./data"
dateDownloaded <- date()
dateDownloaded
head(IdahoHousing)
IH <- read.csv("./data/IdahoHousing.csv")
IH <- read.csv("./quiz1/IdahoHousing.csv")
head(IH)
head(IH$val)
head(IH$VAL)
length(IH$VAL)
nrow(IH$VAL)
num(IH$VAL)
count(IH$VAL)
ncol(IH$VAL)
count(IH$VAL)
counts(IH$VAL)
length(IH$VAL)
levels(IH$VAL)
rane(IH$VAL)
range(IH$VAL)
range(as.vector(IH$VAL)
)
range(as.vector(IH$VAL[!is.na]))
?na
?`na`
?as.na
?if.na
?is.na
range(as.vector(IH$VAL[!is.na]))
range(as.vector(IH$VAL[,!is.na]))
range(as.vector(IH$VAL[!is.na(IH$VAL)]))
length(as.vector(IH$VAL[!is.na(IH$VAL)] == 24))
length(as.vector(IH$VAL[!is.na(IH$VAL)] == 1))
length(as.vector(IH$VAL == 1))
length(as.vector(IH$VAL == 24))
length(IH$VAL == 24)
IH$VAL == 24
sum(IH$VAL == 24)
class(IH)
IH <- read.table("./quiz1/IdahoHousing.csv", sep = ",", header = TRUE)
head(IH)
class(IH)
library(data.table)
IH <- read.table("./quiz1/IdahoHousing.csv", sep = ",", header = TRUE)
class(UH)
class(IH)
tables()
IH <- read.table("./quiz1/IdahoHousing.csv", sep = ",", header = TRUE)
## Quiz 1 Getting and Cleaning Data
setwd("C:/Users/jeffthatcher/Cloud Drive/RRepos/GetCleanData")
library(data.table)
if(!file.exists("quiz1")) {dir.create("quiz1")}
fileUrl <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv"
download.file(fileUrl, destfile = "./quiz1/IdahoHousing.csv") #curl is necessary for MAC users getting data from https
list.files("./quiz1") # sort of like the ls() command, shows the files in the directory "./data"
IH <- read.table("./quiz1/IdahoHousing.csv", sep = ",", header = TRUE)
head(IH$VAL)
table()
IH[IH$VAL=="24",]
IH[IH$VAL==24]
IH[IH$VAL==24,]
f <- IH[IH$VAL==24,]
sum(f)
a <- IH$VAL == 24
a <- IH$VAL[!is.na(IH$VAL)] == 24
head(a)
sum(a)
head(IH$FES)
fileUrl <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FDATA.gov_NGAP.xlsx"
download.file(fileUrl, destfile = "./quiz1/natgas.xlsx", mode="wb")
list.files("./quiz1") # sort of like the ls() command, shows the files in the directory "./data"
library(xlsx)
library(xlsx)
fileUrl <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FDATA.gov_NGAP.xlsx"
download.file(fileUrl, destfile = "./quiz1/natgas.xlsx", mode="wb")
list.files("./quiz1") # sort of like the ls() command, shows the files in the directory "./data"
NG <- read.xlxs("./quiz1/natgas.xlxs", sep = ",", header = TRUE)
NG <- read.xlsx("./quiz1/natgas.xlxs", sep = ",", header = TRUE)
NG <- read.xlsx("./quiz1/natgas.xlsx", sheetIndex=1, header=TRUE)
?read.xlsx
c(7:15)
dat <- read.xlsx("./quiz1/natgas.xlsx", sheetIndex=1,
startRow=18, endRow=23, colIndex=c(7:15),
header=TRUE)
head(dat)
sum(dat$Zip*dat$Ext,na.rm=T)
library(XML)
fileUrl <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml"
doc <- xmlTreeParse(fileUrl, useInternal=TRUE) # loads the document into R memory for parsing
doc
fileUrl <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml"
doc <- xmlTreeParse(fileUrl, useInternal=TRUE) # loads the document into R memory for parsing
doc <- htmlTreeParse(fileUrl, useInternal=TRUE) # loads the document into R memory for parsing
doc
rootNode <- xmlRoot(doc) # weapper element for entire XML document
xmlName(rootNode) # returns the name of the XML
doc
doc <- htmlTreeParse(fileUrl, useInternal=TRUE) # loads the document into R memory for parsing
doc <- xmlTreeParse(fileUrl, useInternal=TRUE) # loads the document into R memory for parsing
fileUrl <- "http://www.w3schools.com/xml/simple.xml"
doc <- xmlTreeParse(fileUrl, useInternal=TRUE) # loads the document into R memory for parsing
rootNode <- xmlRoot(doc) # wrapper element for entire XML document
?xmlTreeParse
fileUrl <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv"
download.file(fileUrl, destfile = "./quiz1/IdahoHousing.csv")
list.files("./quiz1") # sort of like the ls() command, shows the files in the directory "./data"
?fread
DT <- fread("./quiz1/IdahoHousing.csv", sep = ",", header = TRUE)
head(DT)
DT$pwgtp15
str(DT$pwgtp15)
DT[,mean(pwgtp15),by=SEX]
mean(DT$pwgtp15,by=DT$SEX)
sapply(split(DT$pwgtp15,DT$SEX),mean)
mean(DT[DT$SEX==1,]$pwgtp15); mean(DT[DT$SEX==2,]$pwgtp15)
tapply(DT$pwgtp15,DT$SEX,mean)
library(XML)
## Question 4
library(XML)
fileUrl <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml"
doc <- xmlTreeParse(fileUrl, useInternal=TRUE) # loads the document into R memory for parsing
fileUrl <- "http://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml"
doc <- xmlTreeParse(fileUrl, useInternal=TRUE) # loads the document into R memory for parsing
rootNode <- xmlRoot(doc) # weapper element for entire XML document
xmlName(rootNode) # returns the name of the XML
doc
rootNode <- xmlRoot(doc) # weapper element for entire XML document
xmlName(rootNode) # returns the name of the XML
library(XML)
fileUrl <- "http://www.w3schools.com/xml/simple.xml"
doc <- xmlTreeParse(fileUrl, useInternal=TRUE) # loads the document into R memory for parsing
rootNode <- xmlRoot(doc) # wrapper element for entire XML document
xmlName(rootNode) # returns the name of the XML
names(rootNode) # root node wraps the entire document, this command shows the names of these root nodes
rootNode[[1]] # accesses the first element in the list of rootNodes
## Question 5
fileUrl <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv"
download.file(fileUrl, destfile = "./quiz1/IdahoHousing.csv")
list.files("./quiz1") # sort of like the ls() command, shows the files in the directory "./data"
library(XML)
fileUrl <- "http://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml"
doc <- xmlTreeParse(fileUrl, useInternal=TRUE) # loads the document into R memory for parsing
rootNode <- xmlRoot(doc) # weapper element for entire XML document
xmlName(rootNode) # returns the name of the XML
rootNode[[1]][[1]]
rootNode[[1]]
xmlSApply(rootNode, xmlValue)
?xmlValue
xpathSApply(rootNode, "//zipcode", xmlValue)
zip <- xpathSApply(rootNode, "//zipcode", xmlValue)
x <- zip == 21231
sum(x)
